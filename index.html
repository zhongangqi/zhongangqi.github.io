<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Qi, Zhongang (祁仲昂)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ&hl=en">Google Scholar</a></div>
<div class="menu-item"><a href="https://cn.linkedin.com/in/zhongang-qi-902b1254">LinkedIn</a></div>  

</td>
<td id="layout-content">
<div id="toptitle">
<h1>Qi, Zhongang (祁仲昂) </h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://zhongangqi.github.io/"><img src="photos/mmexport1627630520991.jpg" alt="alt text" width="150px" height="170px" /></a>&nbsp;</td>
<td align="left"><p>I am currently a <b>Principal Researcher</b> at <b>Huawei Noah’s Ark Lab</b>, leading an effort on <font color="#FF5151">multimodal large language model and controllable visual content generation</font>. 
  I received the <font color="black">B.S.</font> and <font color="black">Ph.D</font> degrees in information science and electronic engineering from <b>Zhejiang University</b>, supervised by Prof. <a href="https://www.cs.binghamton.edu/~zhongfei/">Zhongfei (Mark) Zhang</a>. 
  I was a <font color="black">Postdoctoral Researcher</font> with the School of Electrical Engineering and Computer Science, <b>Oregon State University</b>, supervised by Prof. <a href="https://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>. 
  Before joining Huawei, I was a <font color="black">Principal Researcher</font> in <b> <a href="https://arc.tencent.com/en/index">ARC Lab</a>, Tencent PCG </b>.<br /> <br />
E-mail: <a href="mailto:zhongangqi@gmail.com">zhongangqi[at]gmail.com</a></p>
</td></tr></table>
  
<h2>Selected Publications </h2>
<h3>Multimodal LLM & Multimodal/Cross-modal Learning</h3>
  <ol>
    <li><p>Yinan Zhou, Yuxin Chen, Haokun Lin, Shuyu Yang, Li Zhu, <b>Zhongang Qi</b>*, Chen Ma, Ying Shan, "DOGE: Towards Versatile Visual Document Grounding and Referring", in arXiv:2411.17125. (*Corresponding author) [<a href="https://arxiv.org/abs/2411.17125">pdf</a>]</p>
</li>
    <li><p>Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, <b>Zhongang Qi</b>, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, Weiming Hu, "mR<sup>2</sup>AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA", in arXiv:2411.15041. [<a href="https://arxiv.org/abs/2411.15041">pdf</a>]</p>
</li>
    <li><p>Ye Liu, Zongyang Ma, <b>Zhongang Qi</b>*, Yang Wu, Ying Shan, Chang Wen Chen, "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding", in NeurIPS 2024. (*Corresponding author) [<a href="https://arxiv.org/abs/2409.18111">pdf</a>]</p>
</li>
     <li><p>Tao Yang, Yingmin Luo, <b>Zhongang Qi</b>*, Yang Wu, Ying Shan, Chang Wen Chen, "PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with LLM", in arXiv:2406.02884. (*Corresponding author) [<a href="https://arxiv.org/abs/2406.02884">pdf</a>]</p>
</li>
    <li><p>Chaolei Tan, Zihang Lin, Junfu Pu, <b>Zhongang Qi</b>, Wei-Yi Pei, Zhi Qu, Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu, “SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses'', in ACM MM 2024.
</p></li>
    <li><p>Zongyang Ma, Ziqi Zhang, Yuxin Chen, <b>Zhongang Qi</b>, Chunfeng Yuan, Bing Li, Yingmin Luo, Xu Li, Xiaojuan Qi, Ying Shan, Weiming Hu, “EA-VTR: Event-Aware Video-Text Retrieval'', in ECCV 2024.
</p></li>
    <li><p>Yuxin Chen, Zongyang Ma, Ziqi Zhang, <b>Zhongang Qi</b>, Chunfeng Yuan, Bing Li, Junfu Pu, Ying Shan, Xiaojuan Qi, Weiming Hu, “How to Make Cross Encoder a Good Teacher for Efficient Image-Text Retrieval?'', in CVPR 2024.
</p></li>
    <li><p>Ziqi Zhang, Zongyang Ma, Chunfeng Yuan, Yuxin Chen, Peijin Wang, <b>Zhongang Qi</b>, Chenglei Hao, Bing Li, Ying Shan, Weiming Hu, Stephen Maybank, “Chinese Title Generation for Short Videos: Dataset, Metric and Algorithm'', in TPAMI 2024.
</p></li>
    <li><p>Zongyang Ma, Ziqi Zhang, Yuxin Chen, <b>Zhongang Qi</b>, Yingmin Luo, Zekun Li, Chunfeng Yuan, Bing Li, Xiaohu Qie, Ying Shan, Weiming Hu, “Order-Prompted Tag Sequence Generation for Video Tagging'', in ICCV 2023.
</p></li>
    <li><p>Yuxuan Zhao, Jin Ma, <b>Zhongang Qi</b>, Zehua Xie, Yu Luo, Qiusheng Kang, Ying Shan, “VTLayout: A Multi-Modal Approach for Video Text Layout'', in ACM MM 2023.
</p></li>
    <li><p>Yuxin Chen, Zongyang Ma, Ziqi Zhang, <b>Zhongang Qi</b>, Chunfeng Yuan, Ying Shan, Bing Li, Weiming Hu, Xiaohu Qie, Jianping Wu, “ViLEM: Visual-Language Error Modeling for Image-Text Retrieval'', in CVPR 2023.
</p></li>
    <li><p>Yizhen Chen, Jie Wang, Lijian Lin, <b>Zhongang Qi</b>*, Jin Ma, Ying Shan, “Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval'', in AAAI 2023. (*Corresponding author)
</p></li>
    <li><p>Yuxin Chen, Ziqi Zhang, <b>Zhongang Qi</b>, Chunfeng Yuan, Jie Wang, Ying Shan, Bing Li, Weiming Hu, Xiaohu Qie, Jianping Wu, “DARTScore: DuAl-Reconstruction Transformer for Video Captioning Evaluation'', in IEEE Transactions on Circuits and Systems for Video Technology, 2023.
</p></li>
    <li><p>Tao Yang, Fan Wang, Junfan Lin, <b>Zhongang Qi</b>, Yang Wu, Jing Xu, Ying Shan, Changwen Chen, “Toward Human Perception-Centric Video Thumbnail Generation'', in ACM MM 2023.
</p></li>
    <li><p>Di Jin, <b>Zhongang Qi</b>, Yingmin Luo, Ying Shan, “TransFusion: Multi-Modal Fusion for Video Tag Inference via Translation-based Knowledge Embedding'', in ACM MM 2021.
</p></li>
    <li><p>Xiao Wang*, Weirong Ye*, <b>Zhongang Qi</b>, Xun Zhao, Guangge Wang, Ying Shan, Hanzi Wang, “Semantic-Guided Relation Propagation Network for Few-shot Action Recognition'', in ACM MM 2021.
</p></li>
    <li><p>Ziqi Zhang, <b>Zhongang Qi</b>, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng, Weiming Hu, “Open-book Video Captioning with Retrieve-Copy-Generate Network'', in CVPR 2021.
</p></li>
  </ol>

<h3>Controllable Visual Content Generation</h3> 
  <ol>
<li><p>Jiangshan Wang, Junfu Pu, <b>Zhongang Qi</b>*, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, Ying Shan, "Taming Rectified Flow for Inversion and Editing", in arXiv:2411.04746. (*Corresponding author) [<a href="https://arxiv.org/abs/2411.04746">pdf</a>]</p>
</li>
<li><p>Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, <b>Zhongang Qi</b>, Ying Shan, Xi Li, “CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities'', in AAAI 2025. [<a href="https://arxiv.org/abs/2408.13239">pdf</a>]
</p></li>
<li><p>Ziyang Yuan, Mingdeng Cao, Xintao Wang, <b>Zhongang Qi</b>, Chun Yuan, Ying Shan, "CustomNet: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models", in ACM MM 2024. [<a href="https://arxiv.org/abs/2310.19784">pdf</a>]</p>
</li>
<li><p>Zhen Li, Mingdeng Cao, Xintao Wang, <b>Zhongang Qi</b>, Ming-Ming Cheng, Ying Shan, “PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding'', in CVPR 2024.
</p></li>
<li><p>Tao Wu, Xuewei Li, <b>Zhongang Qi</b>*, Di Hu, Xintao Wang, Ying Shan, Xi Li, “SphereDiffusion: Spherical Geometry-aware Distortion Resilient Diffusion Model'', in AAAI 2024. (*Corresponding author)
</p></li>
<li><p>Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, <b>Zhongang Qi</b>, Ying Shan, Xiaohu Qie, “T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models'', in AAAI 2024.
</p></li>
<li><p>Mingdeng Cao, Xintao Wang, <b>Zhongang Qi</b>, Ying Shan, Xiaohu Qie, Yinqiang Zheng, “MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing'', in ICCV 2023.
</p></li>
<li><p>Guangcong Zheng, Xianpan Zhou, Xuewei Li, <b>Zhongang Qi</b>, Ying Shan, Xi Li, “LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation'', in CVPR 2023.
</p></li>
  </ol>

<h3>Explainable AI</h3>
  <ol>
    <li><p>Li Fuxin, <b>Zhongang Qi</b>, Saeed Khorram, Vivswan Shitole, Prasad Tadepalli, Minsuk Kahng, Alan Fern, “From Heatmaps to Structured Explanations of Image Classifiers'', in Applied AI Letters, 2021.
</p></li>
    <li><p>Liangbin Xie, Xintao Wang, Chao Dong, <b>Zhongang Qi</b>, Ying Shan, “Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution'', in NeurIPS 2021.
</p></li>
    <li><p>Mandana Hamidi-Haines, <b>Zhongang Qi</b>, Alan Fern, Li Fuxin, Prasad Tadepalli, “User-Guided Global Explanations for Deep Image Recognition: A User Study'', in Applied AI Letters, 2021.
</p></li>
    <li><p><b>Zhongang Qi</b>, Saeed Khorram, Fuxin Li, “Embedding Deep Networks into Visual Explanations'', in Artificial Intelligence (AIJ), Volume 292, March 2021.
</p></li>
    <li><p><b>Zhongang Qi</b>, Saeed Khorram, Fuxin Li, “Visualizing Deep Networks by Optimizing with Integrated Gradients'', in AAAI 2020.
</p></li>
     <li><p>Ziwen Chen, Wenxuan Wu, <b>Zhongang Qi</b>, Fuxin Li, “Visualizing Point Cloud Classifiers by Curvature Smoothing”, in BMVC 2020.
</p></li>
    <li><p>Mandana Hamidi-Haines, <b>Zhongang Qi</b>, Alan Fern, Fuxin Li, Prasad Tadepalli, “Interactive Naming for Explaining Deep Neural Networks: A Formative Study'', in Joint Proceedings of the ACM IUI 2019 Workshops.
</p></li>
    <li><p><b>Zhongang Qi</b>, Fuxin Li, “Learning Explainable Embeddings for Deep Networks'', in NeurIPS 2017 workshop: Interpreting, Explaining and Visualizing Deep Learning - now what ?.
</p></li>
  </ol>

<h3>Other Vision Tasks</h3>
  <ol>
    <li><p>Li Yang, Chunfeng Yuan, Ziqi Zhang, <b>Zhongang Qi</b>, Yan Xu, Wei Liu, Ying Shan, Bing Li, Weiping Yang, Peng Li, Yan Wang, Weiming Hu, “Exploiting Contextual Objects and Relations for 3D Visual Grounding'', in NeurIPS 2023.
</p></li>
     <li><p>Xuewei Li, Tao Wu, <b>Zhongang Qi</b>, Gaoang Wang, Ying Shan, Xi Li, “SGAT4PASS: Spherical Geometry Aware Transformer for PAnoramic Semantic Segmentation'', in IJCAI 2023.
</p></li>
     <li><p>Lijian Lin, Xintao Wang, <b>Zhongang Qi</b>, Ying Shan, “Accelerating the Training of Video Super-Resolution'', in AAAI 2023.
</p></li>
    <li><p>Xixi Xu, <b>Zhongang Qi</b>*, Jianqi Ma, Honglun Zhang, Ying Shan, Xiaohu Qie, “BTS: A Bi-lingual Benchmark for Text Segmentation in the Wild'', in CVPR 2022. (*Corresponding author)
</p></li>
    <li><p>Xingyi Li, <b>Zhongang Qi</b>, Xiaoli Fern, Fuxin Li, “ScaleNet - Improve CNNs through Recursively Rescaling Objects”, in AAAI 2020.
</p></li> 
     <li><p>Wenxuan Wu, <b>Zhongang Qi</b>, Fuxin Li, “PointConv: Deep Convolutional Networks on 3D Point Clouds'', in CVPR 2019.
</p></li>
  </ol>

<h3>Multimodal & Relational Data Mining</h3>
  <ol>
    <li><p>Dan Zhang, Yangliao Geng, Wenwen Gong, <b>Zhongang Qi</b>, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, Jie Tang, “RecDCL: Dual Contrastive Learning for Recommendation'', in WWW 2024.
</p></li>
    <li><p>Dan Zhang, Wenzheng Feng, Yuandong Wang, <b>Zhongang Qi</b>, Ying Shan, Jie Tang, “Dropconn: Dropout connection based random gnns for molecular property prediction'', in IEEE Transactions on Knowledge and Data Engineering (TKDE), 2023.
</p></li>
    <li><p><b>Zhongang Qi</b>, Tianchun Wang, Guojie Song, Weisong Hu, Xi Li, Zhongfei (Mark) Zhang, “Deep Air Learning: Interpolation, Prediction, and Feature Analysis of Fine-grained Air Quality'', in IEEE Transactions on Knowledge and Data Engineering (TKDE), 2018.
</p></li>
    <li><p>Yi Luo, Guojie Song, Pengyu Li, <b>Zhongang Qi</b>, “Multi-Task Medical Concept Normalization Using Multi-View Convolutional Neural Network'', in AAAI 2018.
</p></li>
    <li><p>Yingming Li, Ming Yang, <b>Zhongang Qi</b>, Zhongfei (Mark) Zhang, “Bayesian Multi-task Relationship Learning with Link Structure'', in IEEE Transactions on Knowledge and Data Engineering (TKDE), 2016.
</p></li>
    <li><p>Chengli Mei, Min Zhang, <b>Zhongang Qi</b>, Qi Bi, “Characterizing and Comparing User Location Preference in an Urban Mobile Network'', in Trustworthy Computing and Services</b>, series Communications in Computer and Information Science, Springer Berlin Heidelberg, 2014. 
</p></li>
    <li><p>Yingming Li, Ming Yang, <b>Zhongang Qi</b>, Zhongfei (Mark) Zhang, “Bayesian Multi-task Relationship Learning with Link Structure'', in ICDM 2013.
</p></li>
    <li><p>Yingming Li*, <b>Zhongang Qi</b>*, Zhongfei (Mark) Zhang, Ming Yang, “Learning with Limited and Noisy Tagging'', in ACM MM 2013. (*Contributed equally to this paper)
</p></li>
    <li><p><b>Zhongang Qi</b>, Ming Yang, Zhongfei (Mark) Zhang, Zhengyou Zhang, “Multi-View Learning from Imperfect Tagging'', in ACM MM 2012.
</p></li>
    <li><p><b>Zhongang Qi</b>, Ming Yang, Zhongfei (Mark) Zhang, Zhengyou Zhang, “Mining Noisy Tagging from Multi-label Space'', in CIKM 2012. 
</p></li>
    <li><p><b>Zhongang Qi</b>, Ming Yang, Zhongfei (Mark) Zhang, Zhengyou Zhang, “Mining Partially Annotated Images'', in KDD 2011.
</p></li>
  </ol>
    
</td>
</tr>
</table>
</body>
</html>
